---
title: "SCINet Newsletter: July 2025"
description: "SCINet Newsletter for July 2025"


sections: 
  - title: Research Spotlight
    fetch: 2025-08-04-nouwakpo-hoque
  - title: News
    subsections:
      - New GPUs available on Atlas
      - New Atlas storage and other upgrades
      - Partition/queue changes on Ceres
      - New nodes on Ceres 
      - Internships update 
      - Rangeland Analysis Platform datasets available via SCINet 
  - title: SCINet Working Groups
    text: Working Groups
    subsections:
      - title: SCINet Working Groups
        text: Working groups overview
  - title: Training
    subsections:
      - Training workshops
      - Coursera
      - Workshop Reports
  - title: Support
    subsections:
      - title: SCINet User Tip
        text: "SCINet User Tip: Requesting a specific node type"
      - SCINet Corner
  - title: Connect

canonical: /posts/2025-08-04-nouwakpo-hoque
---

## News

### New GPUs available on Atlas 

Twelve new graphics processing unit (GPU) nodes with a total of 48 GPUs are now available to all SCINet users on SCINet’s Atlas supercomputer. These nodes each have 4 [NVIDIA L40S GPUs](https://www.nvidia.com/en-us/data-center/l40s/), 64 CPU cores, and 1.5 TB of memory. The L40S GPU features 48 GB of GPU memory and was designed for general-purpose workloads, including deep learning and AI applications. These new nodes are available in the “gpu-l40s” partition/queue on Atlas. Many thanks to the SCINet users who provided early testing and feedback for these new GPUs! 

### New Atlas storage and other upgrades 

Atlas’s July 8 maintenance included replacing Atlas’s original data storage hardware with a completely new data storage system. This new system brings several important improvements, including: 

* Nearly 3.4 times more storage space (increased from ~2.2 PB to ~9.6 PB). 
* All solid-state storage that provides read/write performance superior to the previous storage system. This should be especially beneficial for data-intensive GPU workloads. 
* Significant power and space savings compared to the previous storage system. 


All files in /home and /project were automatically migrated to the new storage system. However, files in /90daydata were not automatically migrated. Instead, the old /90daydata is now available at /90daydata-old (read-only), and /90daydata is now located on the new storage system. If you have any files in /90daydata-old that you wish to retain, please copy them to a location on the new storage system. Otherwise, all files in /90daydata-old will expire as usual after 90 days of inactivity, after which the old storage system will be decommissioned.  

Another change for Atlas made during the maintenance event was updating the GPU nodes’ time limits. To help promote resource sharing, jobs submitted to each of the GPU partitions (“gpu-v100”, “gpu-a100”, “gpu-a100-mig7", and “gpu-l40s”) now have a maximum running time of 2 days.  


### Partition/queue changes on Ceres 

During Ceres’ February 2025 maintenance, a new partition/queue, called “ceres”, was added to the supercomputer. This partition includes all community (i.e., non-priority) nodes and was intended to eventually replace all legacy community partitions. During Ceres’ June maintenance, the multitude of legacy community partitions (“short”, “medium”, “long”, “long60”, etc.) were removed, and nearly all compute jobs on Ceres should now be submitted to the “ceres” partition. Use of this partition has several benefits, including: 

* Placing all Ceres community nodes into a single partition will result in shorter wait times and better cluster utilization.  
* A simpler partition scheme makes Ceres easier and less confusing to use.  
* The “ceres” partition is analogous to the “atlas” partition on Atlas and makes the user experience on both systems more similar.  

 Please note that the “ceres” partition has a default job time of 2 hours. This will help avoid long wait times in the queue due to implicitly requesting far more time than a job needs, but it also means that if your job requires more than 2 hours, you will need to explicitly request more time. 

Also note that compute jobs in the “ceres” partition are limited to 3 weeks of run time by default. If you require very long run times, you may submit jobs using the “long” QOS (“quality of service”) specification. This will allow jobs to run for up to 60 days, but they will be limited to using no more than 144 CPU cores. 

For more information, please see the [SCINet website](https://scinet.usda.gov/guides/use/partitions-queues) and [the earlier announcement](https://scinet.usda.gov/news/newsletter/2025-04#an-improved-queuepartition-scheme-for-ceres) in our April newsletter. 

### New nodes on Ceres 

The Ceres maintenance in June included a significant computing hardware upgrade for the supercomputer. All nodes on Ceres that were purchased in 2019 were decommissioned and replaced with 50 new nodes, each of which features 256 logical CPU cores and 2.3 TB of memory. These new nodes, along with the 20 new nodes added last year, not only have far more CPU cores than previous nodes, they also offer more memory than the earlier “high memory” nodes on Ceres. These nodes are available to all users in the “ceres” partition/queue and will, we hope, open up new computing opportunities for SCINet users. 

### Internships update 

Our 2025 AI-COE/SCINet Graduate Student Internships Program summer interns are busy wrapping up their research projects with their ARS mentors!  

We had 15 graduate students participate in summer or spring internships with ARS researchers this year. This year’s interns were affiliated with the New College of Florida, the University of Florida, and North Carolina State University. Participants were selected by their host institutions in part because of their expertise in computer science, machine learning, and data science, and were paired with ARS researchers for projects that could benefit from the intern’s computational skills. 

To recognize the accomplishments of these students, we are holding an internships research symposium on Monday, August 11, 2025 from 1 – 4 PM ET.  

Many thanks to the students who dedicated their time and hard work to these internships, the ARS scientists who volunteered to serve as internship mentors, and the universities who have partnered with us for this internships program. 

 

### Rangeland Analysis Platform datasets available via SCINet  

The [Rangeland Analysis Platform](https://rangelands.app) (RAP) provides remote- sensing- based vegetation cover and production estimates across the continental United States (1986-present). RAP data are helping to support producers through forage production and grazing management tools and to address some of the most pressing issues facing U.S. rangelands such as woody plant encroachment, invasive plants, drought, wind erosion, and fire. A host of federal agencies, organizations, scientists, and others are already [using RAP](https://rangelands.app/bibliography) to inform management strategies and monitor outcomes.  

ARS scientists and collaborators can now access the RAP data on SCINet’s Ceres supercomputer through the Geospatial Common Data Library (GeoCDL) API. Currently, the annual 30-m vegetation cover (perennial forbs and grasses, annual forbs and grasses, shrubs, trees, litter, and bare ground) and annual herbaceous and perennial herbaceous production data as well as the 16-day annual and perennial herbaceous production data are available on Ceres.  Soon we will also have a 10-m data product which includes the vegetation cover groups we have in the 30-m plus new invasive annual grass, sagebrush, pinyon-juniper, and canopy gap layers available through the GeoCDL API. 

## SCINet Working Groups

SCINet working groups (WGs) support ARS researchers and their collaborators in using scientific computing methods and SCINet computational resources in their research. Common WG activities include hosting recurring virtual meetings and webinars, organizing training events, and participating in collaborative research or software development projects.  

Current Working Groups

* [Ag100Pest Initiative (subgroup of AGR)](https://scinet.usda.gov/research/working-groups/ag100pest)
* [Arthropod Genomics Research (AGR) Working Group](https://scinet.usda.gov/research/working-groups/arthropods)
* [Breeding AI and ML Working Group](https://scinet.usda.gov/research/working-groups/breeding)
* [Geospatial Research Working Group](https://scinet.usda.gov/research/working-groups/geospatial)
* [Microbiome Working Group](https://scinet.usda.gov/research/working-groups/microbiome)
* [SCINet-Longterm Agroecosystem Research (LTAR) Phenology Working Group](https://scinet.usda.gov/research/working-groups/LTARphenology)
* [Protein Function and Phenotype Prediction Working Group](https://scinet.usda.gov/research/working-groups/proteinfunction)
* [Translational Omics Working Group](https://scinet.usda.gov/research/working-groups/omics) 

If you are interested in creating a working group, please compile the following: 

* The working group’s name 
* A description of the working group including its purpose and goals 
* Contact information for people to reach out to if they want to learn more about or join the working group. 

Send this information to the SCINet office at [ARS-SCINet-Office@usda.gov](mailto:ARS-SCINet-Office@usda.gov). 



## Training

### Training workshops

#### Practicum AI  

Lead: Research Computing team at the University of Florida 

Developed and presented by the University of Florida and customized for USDA-ARS with funding from ARS’s AI-COE, Practicum AI is a hands-on applied artificial intelligence curriculum intended for learners with limited coding and math background. The goal of this workshop series is to provide a streamlined introduction to the basic tools needed to train and use deep learning models. After completing these workshops, you will be ready to start experimenting with AI and to take more in-depth training, such as SCINet’s machine learning for science workshop series. The Carpentries workshop series offered in October (see below) will also provide a deeper exploration of several of the topics covered in this Practicum AI series. 

Upcoming Practicum AI courses: 

* [**Computing for AI**](https://scinet.usda.gov/events/practicum-ai/computing): August 19, 2025, 2-5 PM ET 
* [**Python for AI**](https://scinet.usda.gov/events/practicum-ai/python): August 27 & 28, 2025, 2-5 PM ET 
* [**Deep Learning Foundations**](https://scinet.usda.gov/events/practicum-ai/deep-learning): September 3 & 4, 2025, 2 – 5 PM ET 

 
Please note that the first two courses in this series cover tools and programming skills that are needed to use and train AI models. Course three (“Deep Learning Foundations”) will cover actual model training and implementation. 

[To register, please complete this registration form.](https://forms.office.com/g/LRgw4gGWKq) 

#### RNA-seq Analysis with Galaxy   

October 6 & 8, 2025, 1-5 PM ET  

Leads: Genome Informatics Facility at Iowa State University and SCINet Office  

In this workshop, participants will work through a complete RNA-seq analysis using SCINet’s Galaxy interface. Participants will learn how to create workflows in Galaxy to:
* Upload and process RNA-seq data.
* Perform quality control, alignment, and expression quantification.
* Identify differentially expressed genes using DESeq2.  

We will also explore ways in which you can share your history and workflows with other SCINet users in Galaxy and how it can be a powerful tool for collaborations.  

[To register, please complete this registration form.](https://forms.office.com/g/HT6C9ad6CL)

#### Carpentries 

Leads: Keo Corak (ARS Computational Biologist), Amisha Poret-Peterson (ARS Research Microbiologist), and Steven Schroeder (ARS Computational Biologist) 

The SCINet Office, in collaboration with ARS’s certified Carpentries instructors, is offering a Carpentries workshop that will teach participants the Unix command line, version control with Git, and Python programming. The workshop will span two weeks:  

* **Unix command line and version control with Git**: October 27 & 29, 2025, 1 – 5 PM ET  
* **Programming with Python**: November 3 & 5, 2025, 1 – 5 PM ET  

This workshop will provide an interactive, hands-on experience that will help you learn valuable skills for data management and analysis. Please note that you may register for either week 1, week 2, or both, depending on which skills you’d like to learn. 


[To register for the Unix, Git, and Python Carpentries Workshop, please fill out this form.](https://forms.office.com/g/viLFLk2sch) 


### Coursera
{% include layout/newsletter/subsection section="/training/2025_coursera" %}


### Workshop Reports 
#### Foundations in bioinformatics workshop series 

Leads: Genome Informatics Facility at Iowa State University and SCINet Office  

Our **Foundations in bioinformatics** workshop series was held this past quarter and consisted of six workshops:  

* [**Introduction to Bioinformatics**](https://scinet.usda.gov/events/2025-04-15-intro): April 15, 2025 
* [**Introduction to HPC Environments and Project Management and Organization**](https://scinet.usda.gov/events/2025-04-17-environments): April 17, 2025 
* [**Data Preparation and Quality Assessment in Genome Assembly**](https://scinet.usda.gov/events/2025-04-29-data): April 29, 2025 
* [**Genome Assembly Validation and Improvement**](https://scinet.usda.gov/events/2025-05-01-validation): May 1, 2025 
* [**Introduction to RNA-seq Analysis**](https://scinet.usda.gov/events/2025-05-13-rna-seq): May 13, 2025 
* [**RNA for Genome Annotation and Reproducibility in Bioinformatics**](https://scinet.usda.gov/events/2025-05-15-reproducibility): May 15, 2025 

Throughout this series, participants gained hands-on experience in command-line tools for assessing the quality of sequencing data, genome assembly, annotation, and RNA-seq analysis. They also learned how to navigate high-performance computing environments and manage biological data effectively for reproducible research. Due to the high interest in this series, we will be offering these workshops again. [Click here to join our waitlist!](https://forms.office.com/g/8qFLk99g5c) 

 

#### From reads to variants: a pipeline for variant calling using DeepVariant  

Leads: Sheina Sim (ARS Research Biologist), Craig Carlson (ARS Research Geneticist), and Haley Arnold (SCINet/AI-COE fellow) 

This workshop provided a hands-on walkthrough of a variant-calling workflow using DeepVariant, a deep-learning based model for highly accurate variant calling. Participants learned how to process whole-genome sequencing data from multiple individuals by trimming and filtering raw reads, mapping to a reference assembly, and calling variants per individual. Participants also learned how to merge and filter variant call form (.vcf) files. The workshop included step-by-step guidance and best practices for variant calling on the SCINet high-performance computing clusters.  


[Click here to access this workshop’s materials and recording.](https://scinet.usda.gov/events/2025-06-03-deepvariant#from-reads-to-variants)  


#### Automate your SCINet pipeline with Snakemake 

Lead: Aaron Yerke (SCINet/AI-COE fellow) 

This workshop introduced participants to Snakemake, a popular workflow management tool that supports documentation, organization, and reproducibility of computational workflows. Participants learned the basics of a Snakemake workflow and how to use it on SCINet clusters through a live demonstration of the pipeline. There was also an additional day of office hours for one-on-one integration of Snakemake into individual workflows/projects.  

 
[Click here to access this workshop’s materials.](https://scinet.usda.gov/events/2025-06-11-snakemake#automate-your-scinet-pipeline-with-snakemake) 


{% include layout/newsletter/subsection section="/training/2025_improve" %}


## Support

{% include layout/newsletter/subsection section="/support/2025_getting_started" %}

{% include layout/newsletter/subsection section="/support/2025_support_email" %}

### SCINet User Tip
**Requesting a specific node type**
{:.subheader}

SCINet’s Ceres supercomputer has a variety of node types, each with different capabilities. For example, some nodes have AMD Epyc processors, others have Intel Xeon processors. In many cases, you can simply submit your compute jobs to the “ceres” partition/queue without worrying about these hardware details. However, what if you need to ensure that your job runs on a particular “kind” of node? For that, you can use Slurm’s “constraints” feature. Constraint flags allow you to tell Slurm exactly which type of node you need. 

As an example, Ceres nodes with AMD Epyc 9754 processors have the “AMD” and “EPYC9754” constraints. To ensure your compute job runs on one (or more) of these nodes, use Slurm’s "--constraint=" option. You could, for instance, add the following to your sbatch script: 

`#SBATCH  --constraint=EPYC9754 `

Again, most of the time, you don’t need to worry about this at all. But if you do need to leverage the capabilities of a particular kind of hardware, it is very handy! Please see the SCINet website for [a table of all node types on Ceres](https://scinet.usda.gov/guides/resources/ceres#technical-overview) and their associated constraint flags. 

**Do you have tips to share? Email them to [ARS-SCINet-Office@usda.gov](mailto:ARS-SCINet-Office@usda.gov) to be included in future newsletters.**

### SCINet Corner

SCINet Corner is a VRSC-moderated virtual space for people to share knowledge, discuss best practices, learn about new opportunities, and explore resources to support progress on their projects.   
 
The next SCINet Corner will be held on August 28, 2025, from 1 – 2 PM ET. August’s event will introduce SCINet’s Galaxy interface, a web-based platform for data-intensive bioinformatics analyses. 

[You can register for this and future SCINet Corners here.](https://forms.office.com/g/tgh4SxRqit)


{% include layout/newsletter/subsection section="/support/2025_question" %}


## Connect

{% include layout/newsletter/subsection section="/connect/2025_connect" %}